
# üìù Representative Publications 
## Multi-modal Generative AI
- Spoken Large Language Model: **AudioGPT (AAAI demo 2024)**, **UniAudio**, **Make-A-Voice**, **Mega-TTS**, **HiFi-Codec**
- Text-to-Audio Synthesis: **Make-An-Audio (ICML 2023)**
- Text-to-Speech Synthesis: **GenerSpeech (NeurIPS 2022)** for zero-shot text-to-speech, **FastDiff (IJCAI 2022) / ProDiff (ACM-MM 2022a)** for diffusion text-to-speech
- Singing Voice Synthesis: **SingGAN (ACM-MM 2022b) / Multi-Singer (ACM-MM 2021)**


## Multi-modal Language Processing
- Audio-Visual Speech-to-Speech Translation: **TranSpeech (ICLR 2023) / AV-TranSpeech (ACL 2023)**
- Self-Supervised Learning: **Prosody-MAE (ACL 2023)**


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv 2023</div><img src='images/make-an-audio-arch.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

- [AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head.](https://arxiv.org/abs/2304.12995) **Rongjie Huang**, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou Zhao, Shinji Watanabe. **Arxiv, 2023**

- **Academic / Industry Impact**: Our work are promoted by different media and forums, such as [Heart of Machine](https://mp.weixin.qq.com/s/pesuhzQ3cfaz-bhxMew46g), [New Intelligence](https://mp.weixin.qq.com/s/BXLxD0bboWS96iEHGZ9xTQ), and [Twitter](https://twitter.com/_akhaliq/status/1619589070329348096). We have code released at [![](https://img.shields.io/github/stars/AIGC-Audio/AudioGPT?style=social&label=Code+Stars)](https://github.com/AIGC-Audio/AudioGPT) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue)](https://huggingface.co/spaces/AIGC-Audio/AudioGPT).

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2023</div><img src='images/make-an-audio-arch.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

- [Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models.]() **Rongjie Huang**, Jiawei Huang, Dongchao Yang, Yi Ren, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, Zhou Zhao. **ICML, 2023. Hawaii, USA**

- **Academic / Industry Impact**: Our work are promoted by different media and forums, such as [Heart of Machine](https://mp.weixin.qq.com/s/fphIJ13RWRIgGNTwYO06bw), [ByteDance](https://zhuanlan.zhihu.com/p/605228032), and [Twitter](https://twitter.com/_akhaliq/status/1619589070329348096). Code is coming!

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2023</div><img src='images/transpeech.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

- [TranSpeech: Speech-to-Speech Translation With Bilateral Perturbation.](https://arxiv.org/abs/2205.12523) **Rongjie Huang**, Jinglin Liu, Huadai Liu, Yi Ren, Lichao Zhang, Jinzheng He, and Zhou Zhao. **ICLR, 2023. Kigali, Rwanda** 

One of our **continuous efforts to reduce communication barrier**, and we have follow-up works: **Audio-Visual S2T [(MixSpeech, ICCV 2023)](https://arxiv.org/abs/2303.05309), Audio-Visual S2ST [(AV-TranSpeech, ACL 2023)](https://arxiv.org/abs/2305.15403), Multi-modal S2ST, Style-aware S2ST,  Zero-shot S2ST**. Code released: [![](https://img.shields.io/github/stars/Rongjiehuang/TranSpeech?style=social&label=Code+Stars)](https://github.com/Rongjiehuang/TranSpeech). 

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2022</div><img src='images/generspeech.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

- [GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech.](https://arxiv.org/abs/2205.07211) **Rongjie Huang**, Yi Ren, Jinglin Liu, Chenye Cui, and Zhou Zhao. **NeurIPS, 2022. New Orleans, USA**

The first **zero-shot** TTS generalizable to unseen speaker, emotion, and prosody! Media coverage: [PaperWeekly](https://mp.weixin.qq.com/s/Mp181vfq24m1HqgJqbMnlg), [Speech Home](https://mp.weixin.qq.com/s/EXdfb0DUTbB6OHbjDS2u7g). Code released: [![](https://img.shields.io/github/stars/Rongjiehuang/GenerSpeech?style=social&label=Code+Stars)](https://github.com/Rongjiehuang/GenerSpeech). 


</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICJAI 2022</div><img src='images/fastdiff.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

- [FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis.](https://arxiv.org/abs/2204.09934) **Rongjie Huang**, Max W.Y. Lam, Jun Wang, Dan Su, Dong Yu, Yi Ren, and Zhou Zhao. **IJCAI, 2022(oral). Vienna, Austria** 

One of our **continuous efforts in generative modeling**, and we have follow-up works: **FastDiff 2, ProDiff**. We release a **diffusion text-to-speech pipeline** [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue)](https://huggingface.co/spaces/Rongjiehuang/ProDiff) using **ProDiff** [![](https://img.shields.io/github/stars/Rongjiehuang/prodiff?style=social&label=Code+Stars)](https://github.com/Rongjiehuang/prodiff) and **FastDiff** [![](https://img.shields.io/github/stars/Rongjiehuang/FastDiff?style=social&label=Code+Stars)](https://github.com/Rongjiehuang/FastDiff). Our work are promoted by different media and forums, such as [Tencent AI Lab](https://mp.weixin.qq.com/s/GmLzLw3GnDsK0OuUpgEySQ), [Speech Home](https://mp.weixin.qq.com/s/BWf_uZdG0icWk5odChxhuA), and [Twitter](https://twitter.com/_akhaliq/status/1517308526691065856), which is a [Trending Project at both Github and Paperwithcode.](https://twitter.com/pythontrending/status/1528332486257819651) 

</div>
</div>

# Full Publication List
* denotes co-first authors, # denotes co-supervised

# 2024

- [AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head.](https://arxiv.org/abs/2304.12995) **Rongjie Huang**, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou Zhao, Shinji Watanabe. **AAAI demo, 2024**

- [StyleSinger: Style Transfer for Out-Of-Domain Singing Voice Synthesis.]() Yu Zhang#, **Rongjie Huang**, Ruiqi Li, Jinzheng He, Yan Xia, Feiyang Chen, Xinyu Duan, Baoxing Huai, Zhou Zhao. **AAAI, 2024.**


# 2023

- [Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models.]() **Rongjie Huang**, Jiawei Huang, Dongchao Yang, Yi Ren, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, Zhou Zhao. **ICML, 2023. Hawaii, USA**

- [UniAudio: An Audio Foundation Model Toward Universal Audio Generation.](https://arxiv.org/abs/2310.00704) Dongchao Yang, Jinchuan Tian, Xu Tan, **Rongjie Huang**, Songxiang Liu, Xuankai Chang, Jiatong Shi, Sheng Zhao, Jiang Bian, Xixin Wu, Zhou Zhao, Shinji Watanabe, Helen Meng. **Arxiv, 2023**

- [Mega-TTS: Zero-Shot Text-to-Speech at Scale with Intrinsic Inductive Bias.]() Ziyue Jiang, Yi Ren, Zhenhui Ye, Jinglin Liu, Chen Zhang, Qian Yang, Shengpeng Ji, **Rongjie Huang**, Chunfeng Wang, Xiang Yin, Zejun Ma, Zhou Zhao. **Arxiv**

- [Make-An-Audio 2: Improving Text-to-Audio with Dual Text Information Representation.]() Jiawei Huang#, Yi Ren, **Rongjie Huang**, Dongchao Yang, Zhenhui Ye, Chen Zhang, Jinglin Liu, Xiang Yin, Zejun Ma, Zhou Zhao. **Arxiv, 2023**

- [TranSpeech: Speech-to-Speech Translation With Bilateral Perturbation.](https://arxiv.org/abs/2205.12523) **Rongjie Huang**, Jinglin Liu, Huadai Liu, Yi Ren, Lichao Zhang, Jinzheng He, and Zhou Zhao. **ICLR, 2023. Kigali, Rwanda** 

- [AV-TranSpeech: Audio-Visual Robust Speech-to-Speech Translation.]() **Rongjie Huang**, Huadai Liu, Xize Cheng, Yi Ren, Linjun Li, Zhenhui Ye, Jinzheng He, Lichao Zhang, Jinglin Liu, Xiang Yin and Zhou Zhao. **ACL, 2023** 

- [MixSpeech: Cross-Modality Self-Learning with Audio-Visual Stream Mixup for Visual Speech Translation and Recognition.]() Xize Cheng*, Linjun Li*, Tao Jin*, **Rongjie Huang***, Wang Lin, Zehan Wang, Huangdai Liu, Ye Wang, Aoxiong Yin, Zhou Zhao. **ICCV, 2023** 

- [CLAPSpeech: Learning Prosody from Text Context with Contrastive Language-Audio Pre-Training.]() Zhenhui Ye*, **Rongjie Huang**, Yi Ren, Ziyue Jiang, Jinglin Liu, Jinzheng He, Xiang Yin and Zhou Zhao. **ACL, 2023** 

- [UniSinger: Unified End-to-End Singing Voice Synthesis With Cross-Modality Information Matching.]() Zhiqing Hong#, Chenye Cui, **Rongjie Huang**, Lichao Zhang, Jinglin Liu, Jinzheng He, Zhou Zhao. **ACM MM, 2023**

- [AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment.]() Ruiqi Li#, **Rongjie Huang**, Lichao Zhang, Jinglin Liu, Zhou Zhao. **ACL finding, 2023**

- [RMSSinger: Realistic-Music-Score based Singing Voice Synthesis.]() Jinzheng He, Jinglin Liu, Zhenhui Ye, **Rongjie Huang**, Chenye Cui, Huadai Liu, Zhou Zhao. **ACL findingfinding, 2023**

- [FluentSpeech: Stutter-Oriented Automatic Speech Editing with Context-Aware Diffusion Models.]() Ziyue Jiang, Qian Yang, Jialong Zuo, Zhenhui Ye, **Rongjie Huang**, Yi Ren, Zhou Zhao. **ACL finding, 2023**

- [Contrastive Token-Wise Meta-Learning for Unseen Performer Visual Temporal-Aligned Translation.]() Linjun Li, Tao Jin, Xize Cheng, Ye Wang, Wang Lin, **Rongjie Huang**, Zhou Zhao. **ACL finding, 2023**

- [ViT-TTS: Visual Text-to-Speech with Scalable Diffusion Transformer.]() Huadai Liu, **Rongjie Huang**, Xuan Lin, Wenqiang Xu, Maozong Zheng, Hong Chen, Jinzheng He, Zhou Zhao. **EMNLP, 2023**


# 2022

- [GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech.](https://arxiv.org/abs/2205.07211) **Rongjie Huang**, Yi Ren, Jinglin Liu, Chenye Cui, and Zhou Zhao. **NeurIPS, 2022. New Orleans, USA**

- [Prosody-TTS: Self-Supervised Prosody Pretraining with Latent Diffusion For Text-to-Speech.]() **Rongjie Huang**, Chunlei Zhang, Yi Ren, Zhou Zhao, Dong Yu. **ACL finding, 2023**

- [FastDiff 2: Dually Incorporating GANs into Diffusion Models for High-Quality Speech Synthesis.]() **Rongjie Huang**, Yi Ren, Jinglin Liu, Luping Liu, Zhou Zhao. **ACL finding, 2023**

- [FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis.](https://arxiv.org/abs/2204.09934) **Rongjie Huang**, Max W.Y. Lam, Jun Wang, Dan Su, Dong Yu, Yi Ren, and Zhou Zhao. **IJCAI, 2022(oral). Vienna, Austria** 

- [ProDiff: Progressive Fast Diffusion Model for High-Quality Text-to-Speech.]() **Rongjie Huang**, Zhou Zhao, Huadai Liu, Jinglin Liu, and Yi Ren. **ACM MM, 2022. Lisbon, Portugal**

- [M4Singer: a Multi-Style, Multi-Singer and Musical Score Provided Mandarin Singing Corpus.](https://arxiv.org/abs/2205.07211) Lichao Zhang, Ruiqi Li, Shoutong Wang, Liqun Deng, Jinglin Liu, Yi Ren, Jinzheng He, **Rongjie Huang**, Jieming Zhu, Xiao Chen, and Zhou Zhao. **NeurIPS, 2022. New Orleans, USA**

- [VarietySound: Timbre-Controllable Video to Sound Generation via Unsupervised Information Disentanglement.]() Chenye Cui, Yi Ren, Jinglin Liu, **Rongjie Huang**, Zhou Zhao. **ICASSP, 2023**

# 2021

- [Multi-Singer: Fast multi-singer singing voice vocoder with a large-scale corpus.](https://dl.acm.org/doi/abs/10.1145/3474085.3475437) **Rongjie Huang**, Feiyang Chen, Yi Ren, Jinglin Liu, Chenye Cui, and Zhou Zhao. **ACM MM, 2021(oral). Chengdu, China** 
| [**Project**](https://multi-singer.github.io/) | [![](https://img.shields.io/github/stars/Rongjiehuang/multi-singer?style=social&label=Code+Stars)](https://github.com/Rongjiehuang/multi-singer)

- [EMOVIE: A Mandarin Emotion Speech Dataset with a Simple Emotional Text-to-Speech Model.](https://arxiv.org/abs/2106.09317) Chenye Cui, Yi Ren, Jinglin Liu, Feiyang Chen, **Rongjie Huang**, Mei Li, and Zhou Zhao. **Interspeech, 2021**

- [Bilateral Denoising Diffusion Models.](https://arxiv.org/abs/2108.11514) Max W.Y. Lam, Jun Wang, **Rongjie Huang**, Dan Su, Dong Yu. **Preprint**


# 2020 and Prior

- [SingGAN: Generative Adversarial NetWork For High-Fidelity Singing Voice Generation.](https://arxiv.org/abs/2110.07468) **Rongjie Huang**, Chenye Cui, Feiyang Chen, Yi Ren, Jinglin Liu, and Zhou Zhao. **ACM MM, 2022. Lisbon, Portugal** 
| [**Project**](https://singgan.github.io/) 