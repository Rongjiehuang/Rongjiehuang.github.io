**Rongjie Huang (ÈªÑËûçÊù∞)** did my Graduate study at College of Computer Science and Software, [Zhejiang University](https://www.zju.edu.cn/english/), supervised by [Prof. Zhou Zhao](https://person.zju.edu.cn/zhaozhou). I also obtained Bachelor‚Äôs degree at Zhejiang University. During my graduate study, I was lucky to collaborate with the CMU Speech Team led by Prof. [Shinji Watanabe](https://scholar.google.com/citations?user=U5xRA6QAAAAJ), and Audio Research Team at Zhejiang University. I was grateful to intern or collaborate at TikTok, Shanghai AI Lab (OpenGV Lab), Tencent Seattle Lab, Alibaba Damo Academic, with [Yi Ren](https://github.com/RayeRen), [Jinglin Liu](https://github.com/MoonInTheRiver)), [Chunlei Zhang](https://scholar.google.com/citations?user=NCKZGb0AAAAJ) and [Dong Yu](https://scholar.google.com/citations?user=tMY31_gAAAAJ)).

My research interest includes **Multi-Modal Generative AI, Multi-Modal Language Processing, and AI4Science**. I have published **first-author papers** at the top international AI conferences such as **NeurIPS/ICLR/ICML/ACL/IJCAI**. I developed a few well-known Speech/NLP algorithms including:
- AudioGPT, UniAudio: Multitask, Multilingual LLMs
- Make-An-Audio, GenerSpeech: Zero-shot text-guided synthesis
- FastDiff 1/2, ProDiff: AIGC diffusion models
- TranSpeech, and AV-TranSpeech: Multimodal Translation

In 2023, I lead or participate in the following research topics:
- Speech/NLP: multimodal generation and translation
- Large Language Models (LLMs): Audio/Visual
- Diffusion models: Image/Audio/3D

**I co-advise research interns with Alibaba [Qianwen team](https://github.com/QwenLM): always looking for research interns with strong CV/Audio/ML background, feel free to shoot an email if interested.**


# üî• News
- *2023.11*: 2 papers are accepted by AAAI 2024 main / AAAI 2024 demo!
- *2023.10*: I am awarded ByteDance Scholar Fellowship, and Chu Kochen Presidential Scholarship!
- *2023.10*: [UniAudio](https://twitter.com/_akhaliq/status/1710112638422642732) comes out! 
- *2023.09*: One paper is accepted by EMNLP 2023!
- *2023.07*: One paper is accepted by ACM-MM 2023!
- *2023.06*: One paper is accepted by ICCV 2023!
- *2023.05*: 8 papers are accepted by ACL 2023 (main conference and findings)! Thanks to my co-authors!
- *2023.04*: AudioGPT [![](https://img.shields.io/github/stars/AIGC-Audio/AudioGPT?style=social&label=Code+Stars)](https://github.com/AIGC-Audio/AudioGPT) and HiFi-Codec [![](https://img.shields.io/github/stars/yangdongchao/AcademiCodec?style=social&label=Code+Stars)](https://github.com/yangdongchao/AcademiCodec) come out! 
- *2023.04*: One papers is accepted by ICML 2023!
- *2023.02*: Make-An-Audio comes out! Media coverage: [Heart of Machine](https://mp.weixin.qq.com/s/fphIJ13RWRIgGNTwYO06bw), [ByteDance](https://zhuanlan.zhihu.com/p/605228032), and [Twitter](https://twitter.com/_akhaliq/status/1619589070329348096).
- *2023.01*: One papers is accepted by ICLR 2023!
- *2022.09*: Two papers are accepted by NeurIPS 2022!
